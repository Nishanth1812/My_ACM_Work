{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****PyTorch Tutorials****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Importing Modules](#Importing-Modules)\n",
    "2. [Tensor Basics](#Tensor-Basics)\n",
    "3. [Autograd](#Autograd)\n",
    "4. [Backpropagation](#Backpropagation)\n",
    "5. [Gradient Descent](#Gradient-Descent)\n",
    "6. [Training Pipeline](#Training-Pipeline)\n",
    "7. [Linear Regression](#Linear-Regression)\n",
    "8. [Logistic Regression](#Logistic-Regression)\n",
    "9. [Dataset and Dataloader](#Dataset-and-Dataloader)\n",
    "10. [Dataset Transforms](#Dataset-Transforms)\n",
    "11. [Softmax and Crossentropy](#Softmax-and-Crossentropy)\n",
    "12. [Activation Functions](#Activation-Functions)\n",
    "13. [Feed Forward Net](#Feed-Forward-Net)\n",
    "14. [CNN](#CNN)\n",
    "15. [Tensorboard](#Tensorboard)\n",
    "16. [Save & Load Models](#Save--Load-Models)\n",
    "16. [Tensorboard](#Tensorboard)\n",
    "17. [Save & Load Models](#Save--Load-Models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Tensors?**\n",
    "\n",
    "-> A tensor can be defined as a generalised array , which can have any number of dimensions.\n",
    "\n",
    "**What is the main difference between arrays and tensors?**\n",
    "\n",
    "->  Arrays store numbers in multiple dimensions, while tensors are specialized arrays optimized for AI and deep learning, enabling faster computations on GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Basics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.) empty tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([inf])\n",
      "tensor([[0., 0.]])\n",
      "tensor([[[6.4104e-30, 2.1468e-42, 1.4013e-45],\n",
      "         [0.0000e+00, 1.4013e-45, 0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "t_em_1=torch.empty(1)  # 1D tensor \n",
    "print(t_em_1)\n",
    "\n",
    "t_em_2=torch.empty(1,2) # 2D tensor \n",
    "print(t_em_2)\n",
    "\n",
    "t_em_3=torch.empty(1,2,3) # 3d Tensor\n",
    "print(t_em_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) tensor containing random values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9270])\n",
      "tensor([[0.3347, 0.6518]])\n",
      "tensor([[[0.8908, 0.8825, 0.8191],\n",
      "         [0.5871, 0.6928, 0.9165]]])\n",
      "tensor([-1.4969])\n"
     ]
    }
   ],
   "source": [
    "t_rand_1=torch.rand(1) # 1D tensor containing random values between 0 and 1\n",
    "print(t_rand_1)\n",
    "\n",
    "t_rand_2=torch.rand(1,2) # 2D tensor containing random values between 0 and 1\n",
    "print(t_rand_2)\n",
    "\n",
    "t_rand_3=torch.rand(1,2,3) # 3D tensor containing random values between 0 and 1\n",
    "print(t_rand_3)\n",
    "\n",
    "t_randn_1=torch.randn(1)  # 1D tensor containing random variable with mean 0 and variance 1 (nummbers will be between -infinity and infinity)\n",
    "print(t_randn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.) tensor initialized with zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t_zero_2=torch.zeros(1,2) # 2D tensor initialised with zeroes\n",
    "print(t_zero_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.) tensor initialized with ones**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_one_2=torch.ones(1,3) # A 2D tensor initialised with ones\n",
    "print(t_one_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** By default the datatype of a tensor is float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.)creating tensors of specific data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], dtype=torch.float64)\n",
      "tensor([[1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 1D tensor of float64 type\n",
    "\n",
    "t_dtype_1=torch.zeros(1, dtype=torch.float64)\n",
    "print(t_dtype_1)\n",
    "\n",
    "# 2D tensor of int type (int32)\n",
    "\n",
    "t_dtype_2=torch.ones(1,2,dtype=torch.int)\n",
    "print(t_dtype_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can see the size of a tensor by using the .size() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.) checking the size of a tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "t_size_1=torch.rand(1,3) # A 2d tensor containing random valeus and having a size of (1,3) \n",
    "\n",
    "# Checking the size \n",
    "print(t_size_1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.) creating a tensor from a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000e+00, 5.3400e+01, 5.4450e+03, 3.5500e+02])\n"
     ]
    }
   ],
   "source": [
    "data=[1.5,53.4,5445,355]\n",
    "\n",
    "# Creatin the tensor \n",
    "t_list_1=torch.tensor(data)\n",
    "print(t_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing basic operations using tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In pytorch every function that contains a trailing \"_\" performs an inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating the base tensors which will be used to perform the operations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(2,2)\n",
    "y=torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.) addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE TENSORS\n",
      "\n",
      "Base Tensor 1:\n",
      " tensor([[0.7874, 0.3351],\n",
      "        [0.1211, 0.7048]])\n",
      "Base Tensor 2:\n",
      " tensor([[1.7649, 0.3626],\n",
      "        [0.5242, 1.1341]])\n",
      "\n",
      "RESULTANT TENSORS\n",
      "\n",
      "Resultant Tensor 1 :\n",
      "  tensor([[1.7649, 0.3626],\n",
      "        [0.5242, 1.1341]])\n",
      "Resultant Tensor 2 : \n",
      " tensor([[1.7649, 0.3626],\n",
      "        [0.5242, 1.1341]])\n",
      "Resultant Tensor 3 : \n",
      " tensor([[1.7649, 0.3626],\n",
      "        [0.5242, 1.1341]])\n"
     ]
    }
   ],
   "source": [
    "# Direct addition\n",
    "z_add=x+y \n",
    "\n",
    "# Addition using pytorch function\n",
    "z_add_func=torch.add(x,y)\n",
    "\n",
    "# Inplace addition \n",
    "y_add_test=y #Copying y to another variable to keep the base tensor the saem \n",
    "y_add_test.add_(x)\n",
    "\n",
    "# Printing the base tensors\n",
    "\n",
    "print(\"BASE TENSORS\\n\")\n",
    "print(\"Base Tensor 1:\\n\",x)\n",
    "print(\"Base Tensor 2:\\n\",y)\n",
    "\n",
    "# Printing the resultant tensors \n",
    "\n",
    "print(\"\\nRESULTANT TENSORS\\n\")\n",
    "print(\"Resultant Tensor 1 :\\n \",z_add) # direct addition\n",
    "print(\"Resultant Tensor 2 : \\n\",z_add_func) # using pytorch add funiton\n",
    "print(\"Resultant Tensor 3 : \\n\",y_add_test) # using inplace addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) subraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE TENSORS\n",
      "\n",
      "Base Tensor 1:\n",
      " tensor([[0.7874, 0.3351],\n",
      "        [0.1211, 0.7048]])\n",
      "Base Tensor 2:\n",
      " tensor([[0.9775, 0.0275],\n",
      "        [0.4031, 0.4293]])\n",
      "\n",
      "RESULTANT TENSORS\n",
      "\n",
      "Resultant Tensor 1 :\n",
      "  tensor([[-0.9775, -0.0275],\n",
      "        [-0.4031, -0.4293]])\n",
      "Resultant Tensor 2 : \n",
      " tensor([[-0.9775, -0.0275],\n",
      "        [-0.4031, -0.4293]])\n",
      "Resultant Tensor 3 : \n",
      " tensor([[0.9775, 0.0275],\n",
      "        [0.4031, 0.4293]])\n"
     ]
    }
   ],
   "source": [
    "# Direct Subraction\n",
    "z_sub=x-y \n",
    "\n",
    "# Addition using pytorch function\n",
    "z_sub_func=torch.sub(x,y)\n",
    "\n",
    "# Inplace Subraction \n",
    "y_sub_test=y #Copying y to another variable to keep the base tensor the saem \n",
    "y_sub_test.sub_(x)\n",
    "\n",
    "# Printing the base tensors\n",
    "print(\"BASE TENSORS\\n\")\n",
    "print(\"Base Tensor 1:\\n\",x)\n",
    "print(\"Base Tensor 2:\\n\",y)\n",
    "\n",
    "# Printing the resultant tensors \n",
    "\n",
    "print(\"\\nRESULTANT TENSORS\\n\")\n",
    "print(\"Resultant Tensor 1 :\\n \",z_sub) # direct Subraction\n",
    "print(\"Resultant Tensor 2 : \\n\",z_sub_func) # using pytorch sub function\n",
    "print(\"Resultant Tensor 3 : \\n\",y_sub_test) # using inplace Subraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.) multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE TENSORS\n",
      "\n",
      "Base Tensor 1:\n",
      " tensor([[0.7874, 0.3351],\n",
      "        [0.1211, 0.7048]])\n",
      "Base Tensor 2:\n",
      " tensor([[0.7696, 0.0092],\n",
      "        [0.0488, 0.3026]])\n",
      "\n",
      "RESULTANT TENSORS\n",
      "\n",
      "Resultant Tensor 1 :\n",
      "  tensor([[0.7696, 0.0092],\n",
      "        [0.0488, 0.3026]])\n",
      "Resultant Tensor 2 : \n",
      " tensor([[0.7696, 0.0092],\n",
      "        [0.0488, 0.3026]])\n",
      "Resultant Tensor 3 : \n",
      " tensor([[0.7696, 0.0092],\n",
      "        [0.0488, 0.3026]])\n"
     ]
    }
   ],
   "source": [
    "# Direct Multiplication\n",
    "z_mul=x*y \n",
    "\n",
    "# Addition using pytorch function\n",
    "z_mul_func=torch.mul(x,y)\n",
    "\n",
    "# Inplace Multiplication \n",
    "y_mul_test=y #Copying y to another variable to keep the base tensor the saem \n",
    "y_mul_test.mul_(x)\n",
    "\n",
    "# Printing the base tensors\n",
    "print(\"BASE TENSORS\\n\")\n",
    "print(\"Base Tensor 1:\\n\",x)\n",
    "print(\"Base Tensor 2:\\n\",y)\n",
    "\n",
    "# Printing the resultant tensors \n",
    "\n",
    "print(\"\\nRESULTANT TENSORS\\n\")\n",
    "print(\"Resultant Tensor 1 :\\n \",z_mul) # direct Multiplication\n",
    "print(\"Resultant Tensor 2 : \\n\",z_mul_func) # using pytorch mul function\n",
    "print(\"Resultant Tensor 3 : \\n\",y_mul_test) # using inplace Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.)division**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE TENSORS\n",
      "\n",
      "Base Tensor 1:\n",
      " tensor([[0.7874, 0.3351],\n",
      "        [0.1211, 0.7048]])\n",
      "Base Tensor 2:\n",
      " tensor([[0.9775, 0.0275],\n",
      "        [0.4031, 0.4293]])\n",
      "\n",
      "RESULTANT TENSORS\n",
      "\n",
      "Resultant Tensor 1 :\n",
      "  tensor([[ 1.0230, 36.3828],\n",
      "        [ 2.4808,  2.3293]])\n",
      "Resultant Tensor 2 : \n",
      " tensor([[ 1.0230, 36.3828],\n",
      "        [ 2.4808,  2.3293]])\n",
      "Resultant Tensor 3 : \n",
      " tensor([[0.9775, 0.0275],\n",
      "        [0.4031, 0.4293]])\n"
     ]
    }
   ],
   "source": [
    "# Direct Division\n",
    "z_div=x/y \n",
    "\n",
    "# Addition using pytorch function\n",
    "z_div_func=torch.div(x,y)\n",
    "\n",
    "# Inplace Division \n",
    "y_div_test=y #Copying y to another variable to keep the base tensor the saem \n",
    "y_div_test.div_(x)\n",
    "\n",
    "# Printing the base tensors\n",
    "print(\"BASE TENSORS\\n\")\n",
    "print(\"Base Tensor 1:\\n\",x)\n",
    "print(\"Base Tensor 2:\\n\",y)\n",
    "\n",
    "# Printing the resultant tensors \n",
    "\n",
    "print(\"\\nRESULTANT TENSORS\\n\")\n",
    "print(\"Resultant Tensor 1 :\\n \",z_div) # direct Division\n",
    "print(\"Resultant Tensor 2 : \\n\",z_div_func) # using pytorch div function\n",
    "print(\"Resultant Tensor 3 : \\n\",y_div_test) # using inplace Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.) slicing operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[0.7905, 0.9172, 0.8340, 0.0468, 0.4464, 0.1711],\n",
      "        [0.1757, 0.6912, 0.9022, 0.0520, 0.3623, 0.5038],\n",
      "        [0.8996, 0.8022, 0.5400, 0.5823, 0.1195, 0.3165],\n",
      "        [0.6260, 0.5134, 0.4350, 0.9371, 0.4844, 0.4437],\n",
      "        [0.4337, 0.5514, 0.5513, 0.5181, 0.8524, 0.2866]])\n",
      "\n",
      " Sliced Tensor-1:\n",
      " tensor([0.8340, 0.9022, 0.5400, 0.4350, 0.5513])\n",
      "\n",
      " Sliced Tensor-2:\n",
      " tensor([0.0468, 0.0520])\n"
     ]
    }
   ],
   "source": [
    "x_slice=torch.rand(5,6) # Creating a random 2d tensor to perform slicing \n",
    "\n",
    "t_slice_1=x_slice[:,2] # To get all the rows of the 3rd column \n",
    "t_slice_2=x_slice[:2,3] # To get  the First 2 rows of the 4th column \n",
    "\n",
    "print(\"Original Tensor:\\n\",x_slice)\n",
    "print(\"\\n Sliced Tensor-1:\\n\",t_slice_1) # all rows of the 3rd columns \n",
    "print(\"\\n Sliced Tensor-2:\\n\",t_slice_2) # First 2 rows  of the 4th column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To get the actual value of an element in the tensor , we can use the .item() function. But we can use this method when we have only 1 element in the sliced tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[0.6195, 0.1377],\n",
      "        [0.1856, 0.1210]])\n",
      "\n",
      " 0.12095630168914795\n"
     ]
    }
   ],
   "source": [
    "a_test=torch.rand(2,2)\n",
    "print(\"Original tensor:\\n\",a_test)\n",
    "print(\"\\n\",a_test[1,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.)resizing a tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[0.7621, 0.9147, 0.2647, 0.2112],\n",
      "        [0.8561, 0.2782, 0.0176, 0.9660]])\n",
      "\n",
      "Resized Tensor:\n",
      " tensor([[0.7621, 0.9147],\n",
      "        [0.2647, 0.2112],\n",
      "        [0.8561, 0.2782],\n",
      "        [0.0176, 0.9660]])\n"
     ]
    }
   ],
   "source": [
    "a_test_2=torch.rand(2,4)\n",
    "\n",
    "a_resize_1=a_test_2.view(-1,2) # The -1 value in the first dimension means , that pytorch automatically decides the appropriate dimesnion size \n",
    "\n",
    "print(\"Original Tensor:\\n\",a_test_2)\n",
    "print(\"\\nResized Tensor:\\n\",a_resize_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conversions between numpy arrays and torch tensors***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.) converting a tensor to a numpy array**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3461, 0.5214, 0.0774, 0.2737]]) \n",
      "Data type:  torch.float32\n",
      "\n",
      " [[0.34607476 0.5214479  0.07736278 0.27366966]] \n",
      "Data type:  float32\n"
     ]
    }
   ],
   "source": [
    "a_test_3=torch.rand(1,4)\n",
    "\n",
    "# Converting it into a numpy array \n",
    "\n",
    "a_numpy_1 = a_test_3.numpy()\n",
    "\n",
    "print(a_test_3,\"\\nData type: \",a_test_3.dtype)\n",
    "print(\"\\n\",a_numpy_1,\"\\nData type: \",a_numpy_1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The numpy array and the torch tensor point to the same memory address , so if one is changed , then the other one also gets changed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) converting a numpy array into a tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Array:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Torch tenser\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np_array_1=np.zeros((3,3))\n",
    "print(\"Numpy Array:\\n\",np_array_1)\n",
    "\n",
    "# Converting it into a tensor \n",
    "\n",
    "torch_ten_1=torch.from_numpy(np_array_1) \n",
    "\n",
    "print(\"\\nTorch tenser\\n\",torch_ten_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** To use autograd , we have to specify an argument requires_grad as True\n",
    "\n",
    "**Gradient:** In deep learning , gradients tell us how much we have to change the weights so that the models predictions improve\n",
    "\n",
    "**Back Propagation:** It is the process by which the models continuously changes its weights , based on the gradients to improve the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.) Creating the base tensors on which the operations will be performed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.7600], requires_grad=True)\n",
      "tensor([0.0741], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t_grad_1=torch.tensor([5.76],requires_grad=True)\n",
    "t_grad_2=torch.rand(1,requires_grad=True)\n",
    "\n",
    "print(t_grad_1)\n",
    "print(t_grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) Performing some operations on the tensor to see how the tracking is done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.8341], grad_fn=<AddBackward0>)\n",
      "tensor([5.6859], grad_fn=<SubBackward0>)\n",
      "tensor([0.4270], grad_fn=<MulBackward0>)\n",
      "tensor([77.6940], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Addition \n",
    "\n",
    "t_res_add=torch.add(t_grad_1,t_grad_2) #Main tensor\n",
    "\n",
    "print(t_res_add)\n",
    "\n",
    "# Subraction \n",
    "\n",
    "t_res_sub=torch.sub(t_grad_1,t_grad_2) #Main tensor\n",
    "\n",
    "print(t_res_sub) \n",
    "\n",
    "# Multiplication \n",
    "\n",
    "t_res_mul=torch.mul(t_grad_1,t_grad_2) #Main tensor\n",
    "\n",
    "print(t_res_mul)\n",
    "\n",
    "# Division \n",
    "\n",
    "t_res_div=torch.div(t_grad_1,t_grad_2) #Main tensor\n",
    "\n",
    "print(t_res_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.) Calculating the gradients for each operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To calculate the gradient of a tensor which contains more than 1 value , we have to pass a vector of the same size into the .backward() method as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for addition operation: \n",
      "\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "\n",
      "Gradients for subraction operation: \n",
      "\n",
      "tensor([2.])\n",
      "tensor([0.])\n",
      "\n",
      "Gradients for multiplication operation: \n",
      "\n",
      "tensor([2.0741])\n",
      "tensor([5.7600])\n",
      "\n",
      "Gradients for Division operation: \n",
      "\n",
      "tensor([15.5627])\n",
      "tensor([-1042.2179])\n"
     ]
    }
   ],
   "source": [
    "#  Calculating and printing the gradients for each operation \n",
    "\n",
    "# Addition \n",
    "print(\"Gradients for addition operation: \\n\")\n",
    "t_res_add.backward()\n",
    "print(t_grad_1.grad)\n",
    "print(t_grad_2.grad)\n",
    "\n",
    "# Subraction \n",
    "\n",
    "print(\"\\nGradients for subraction operation: \\n\")\n",
    "t_res_sub.backward()\n",
    "print(t_grad_1.grad)\n",
    "print(t_grad_2.grad)\n",
    "\n",
    "# Multilplication \n",
    "\n",
    "print(\"\\nGradients for multiplication operation: \\n\")\n",
    "t_res_mul.backward()\n",
    "print(t_grad_1.grad)\n",
    "print(t_grad_2.grad) \n",
    "\n",
    "# Division \n",
    "\n",
    "print(\"\\nGradients for Division operation: \\n\")\n",
    "t_res_div.backward()\n",
    "print(t_grad_1.grad)\n",
    "print(t_grad_2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.) Creating gradients for tensors with more than 1 element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7947, -1.1164, -1.6054,  0.7655,  0.1070, -2.0595,  0.1088,  0.2675,\n",
      "         0.7315, -1.4014], requires_grad=True) \n",
      "\n",
      "tensor([ 0.0104, -1.3171, -1.0139, -0.2205,  0.5254,  1.1900, -0.6731, -0.0298,\n",
      "         1.4573, -1.3420], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating base tensors with require_Grad=True\n",
    "\n",
    "t_grad_3=torch.randn(10,requires_grad=True)\n",
    "t_grad_4=torch.randn(10,requires_grad=True)\n",
    "\n",
    "# Printing all the elements of the tensor \n",
    "\n",
    "print(t_grad_3,\"\\n\")\n",
    "print(t_grad_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating different types of vectors of size 10 to calculate the gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use a vector of ones, we'll get the gradients for each element of the tensor\n",
    "v_1=torch.ones(10)\n",
    "v_1\n",
    "\n",
    "# IF we want to get the gradient of a specific element 'n' in the tensor , we use a vector , which contains 2 at the nth position , and zeroes at all other positions \n",
    "\n",
    "v_2=torch.zeros(10)\n",
    "v_2[4]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculating the gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 2., 1., 1., 1., 1., 1.]) \n",
      "\n",
      "tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1.]) \n",
      "\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]) \n",
      "\n",
      "tensor([-0., -0., -0., -0., -1., -0., -0., -0., -0., -0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing a simple operation \n",
    "t_res_test_1=torch.add(t_grad_3,t_grad_4)\n",
    "t_res_test_2=torch.sub(t_grad_3,t_grad_4)\n",
    "\n",
    "# Uaing backward to calculate the gradients of all the elements\n",
    "print(\"Operation\")\n",
    "t_res_test_1.backward(v_1)\n",
    "print(t_grad_3.grad,\"\\n\")\n",
    "print(t_grad_4.grad,\"\\n\")\n",
    "\n",
    "# Zeroing out the gradients for the base tensors to perform another operation \n",
    "\n",
    "t_grad_3.grad=None\n",
    "t_grad_4.grad=None\n",
    "\n",
    "# Using backward to calculate the gradient of the 5th element of the tensor \n",
    "\n",
    "t_res_test_2.backward(v_2)\n",
    "print(t_grad_3.grad,\"\\n\")\n",
    "print(t_grad_4.grad,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.) Testing multiple methods to detach the tensor from pytorch tracking for gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When we don't want pytorch to track the history of tensor operations and track gradients , we can use multiple methods like .requires_grad() , .detach() method or we can put it in a with torch.no_grad() statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Tensor-1: \n",
      " tensor([ 1.1181,  1.9842, -0.5044,  0.8874,  1.1429], requires_grad=True)\n",
      "\n",
      "Test Tensor-2: \n",
      " tensor([-0.1952, -1.1459, -1.0293,  2.1160, -0.9339], requires_grad=True)\n",
      "\n",
      "Test Tensor-3: \n",
      " tensor([-0.6892,  0.6875,  0.8057,  0.5373,  0.6222], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating a test tensor with requires_Grad=True\n",
    "\n",
    "t_grad_5=torch.randn(5,requires_grad=True)\n",
    "t_grad_6=torch.randn(5,requires_grad=True)\n",
    "t_grad_7=torch.randn(5,requires_grad=True)\n",
    "print(\"Test Tensor-1: \\n\",t_grad_5)\n",
    "print(\"\\nTest Tensor-2: \\n\",t_grad_6)\n",
    "print(\"\\nTest Tensor-3: \\n\",t_grad_7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor after using requires_grad method: \n",
      " tensor([ 1.1181,  1.9842, -0.5044,  0.8874,  1.1429])\n",
      "\n",
      "Tensor after deataching: \n",
      " tensor([-0.1952, -1.1459, -1.0293,  2.1160, -0.9339])\n",
      "\n",
      "Printing the tensor before using torch.no_grad(): \n",
      "\n",
      "tensor([-0.6892,  0.6875,  0.8057,  0.5373,  0.6222], requires_grad=True)\n",
      "\n",
      "Tensor after using torch.no_grad(): \n",
      " tensor(1.9634)\n"
     ]
    }
   ],
   "source": [
    "# Removing the tensor from pytorch tracking \n",
    "\n",
    "# 1.) using required_grad() \n",
    "t_grad_5.requires_grad_(False)\n",
    "print(\"Tensor after using requires_grad method: \\n\",t_grad_5)\n",
    "\n",
    "# 2.) Using .detach() method \n",
    "t_grad_6.detach_()\n",
    "print(\"\\nTensor after deataching: \\n\",t_grad_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing the tensor before using torch.no_grad(): \n",
      " tensor([-0.6892,  0.6875,  0.8057,  0.5373,  0.6222], requires_grad=True)\n",
      "\n",
      "Tensor after using torch.no_grad(): \n",
      " tensor(1.9634)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrinting the tensor before using torch.no_grad(): \\n\",t_grad_7)\n",
    "\n",
    "# 3.) Using wtih torch.no_grad():\n",
    "with torch.no_grad():\n",
    "    t_res_nograd_1=t_grad_7.sum()\n",
    "    print(\"\\nTensor after using torch.no_grad(): \\n\",t_res_nograd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can make the gradients 0 by using the method .grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Backpropagation?**\n",
    "\n",
    "-> Backpropagation is a process by which a models improves it accuracy by constantly adjusting its weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is computational graph?**\n",
    "\n",
    "-> A **computational graph** visually represents mathematical operations in a machine-learning model, breaking down complex calculations into smaller, interconnected steps for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the difference between Gradients and local gradients?**\n",
    "\n",
    "-> A gradient is defined as the the overall rate of change of the loss function with respect to a model parameter \n",
    "\n",
    "-> A local gradient is defined as the rate of change of an intermediate nodeâ€™s output with respect to its input in a computational graph.\n",
    "\n",
    "In simple words, we can say that ,\n",
    "\n",
    "-> Gradient determines how much a weight should change to reduce error, whereas a Local Gradient is a small part of this, helping backpropagation compute the full gradient using the chain rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
